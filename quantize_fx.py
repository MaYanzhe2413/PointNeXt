#!/usr/bin/env python3
"""
PointNeXt PyTorch FX é‡åŒ–è„šæœ¬
ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨PyTorch FXè¿›è¡Œå›¾æ¨¡å¼é‡åŒ–
"""

import os
import sys
import torch
import torch.nn as nn
import torch.quantization as quantization
from torch.fx import symbolic_trace
# å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬çš„å¯¼å…¥
try:
    from torch.quantization import get_default_qconfig_mapping
    from torch.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx
except ImportError:
    # PyTorch 1.10åŠä»¥ä¸‹ç‰ˆæœ¬çš„å…¼å®¹æ€§å¯¼å…¥
    try:
        from torch.ao.quantization import get_default_qconfig_mapping
        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx
    except ImportError:
        # æœ€åŸºç¡€çš„é‡åŒ–API
        from torch.quantization.quantize_fx import prepare_fx, convert_fx
        try:
            from torch.quantization.quantize_fx import prepare_qat_fx
        except ImportError:
            prepare_qat_fx = None
        
        def get_default_qconfig_mapping(backend='fbgemm'):
            """å…¼å®¹æ—§ç‰ˆæœ¬çš„qconfig mapping"""
            if backend == 'fbgemm':
                return torch.quantization.get_default_qconfig('fbgemm')
            elif backend == 'qnnpack':
                return torch.quantization.get_default_qconfig('qnnpack')
            else:
                return torch.quantization.get_default_qconfig('fbgemm')

import torch.quantization.observer as observer
import torch.optim as optim
import copy
import numpy as np
import yaml
import argparse
from typing import Dict, Any
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ openpointsåˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'openpoints'))

from openpoints.models import build_model_from_cfg
from openpoints.utils import EasyConfig
from openpoints.dataset import build_dataloader_from_cfg

# å¯¼å…¥PointNeXtä¸­éœ€è¦è·³è¿‡é‡åŒ–çš„å±‚ç±»
try:
    from openpoints.models.layers.subsample import FurthestPointSampling
    from openpoints.models.layers.group import QueryAndGroup, BallQuery, GroupingOperation
    FX_LAYER_IMPORTS_AVAILABLE = True
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥PointNeXtå±‚ç±»ï¼Œå°†ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…æ–¹å¼è·³è¿‡å±‚")
    FurthestPointSampling = None
    QueryAndGroup = None
    BallQuery = None
    GroupingOperation = None
    FX_LAYER_IMPORTS_AVAILABLE = False

# å¯¼å…¥FXè¡¥ä¸
try:
    from fx_subsample_patch import apply_fx_patches
    FX_PATCH_AVAILABLE = True
except ImportError:
    print("âš ï¸ FXè¡¥ä¸ä¸å¯ç”¨")
    FX_PATCH_AVAILABLE = False


class SimplePointNeXtQuantizer:
    """
    ç®€åŒ–çš„PointNeXté‡åŒ–å™¨
    ä½¿ç”¨PyTorch FXè¿›è¡Œå›¾æ¨¡å¼é‡åŒ–
    """
    
    def __init__(self, config_path: str, pretrained_path: str = None):
        """
        åˆå§‹åŒ–é‡åŒ–å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            pretrained_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        """
        self.cfg = EasyConfig()
        self.cfg.load(config_path)
        self.pretrained_path = pretrained_path
        
        # è®¾ç½®é‡åŒ–é…ç½® - å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬
        try:
            self.qconfig_mapping = get_default_qconfig_mapping("fbgemm")
        except:
            # æ—§ç‰ˆæœ¬çš„é‡åŒ–é…ç½®
            self.qconfig_mapping = torch.quantization.get_default_qconfig('fbgemm')
        
        print(f"ğŸ”§ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        print(f"ğŸ“¦ é¢„è®­ç»ƒæ¨¡å‹: {pretrained_path if pretrained_path else 'æ— '}")
        
    def build_model(self) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        print("ğŸ—ï¸  æ„å»ºæ¨¡å‹...")
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  æ¨¡å‹è®¾å¤‡: {device}")
        
        # æ„å»ºæ¨¡å‹
        model = build_model_from_cfg(self.cfg.model)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if self.pretrained_path and os.path.exists(self.pretrained_path):
            print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {self.pretrained_path}")
            checkpoint = torch.load(self.pretrained_path, map_location=device)
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
                
            # ç§»é™¤ä¸åŒ¹é…çš„é”®
            model_keys = set(model.state_dict().keys())
            checkpoint_keys = set(state_dict.keys())
            
            # æ‰¾åˆ°åŒ¹é…çš„é”®
            matched_keys = model_keys.intersection(checkpoint_keys)
            unmatched_model_keys = model_keys - checkpoint_keys
            unmatched_checkpoint_keys = checkpoint_keys - model_keys
            
            print(f"âœ… åŒ¹é…çš„å‚æ•°: {len(matched_keys)}")
            if unmatched_model_keys:
                print(f"âš ï¸  æ¨¡å‹ä¸­æœªåŒ¹é…çš„å‚æ•°: {len(unmatched_model_keys)}")
            if unmatched_checkpoint_keys:
                print(f"âš ï¸  checkpointä¸­æœªåŒ¹é…çš„å‚æ•°: {len(unmatched_checkpoint_keys)}")
            
            # åŠ è½½åŒ¹é…çš„æƒé‡
            filtered_state_dict = {k: v for k, v in state_dict.items() if k in matched_keys}
            model.load_state_dict(filtered_state_dict, strict=False)
        
        # å°†æ¨¡å‹ç§»åˆ°GPU
        model = model.to(device)
        model.eval()
        print(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ: {type(model).__name__}")
        return model

    def qat_train_model(self, model, train_loader, num_epochs=3, lr=0.001):
        """
        QATè®­ç»ƒè¿‡ç¨‹
        
        Args:
            model: å·²ç»å‡†å¤‡å¥½çš„QATæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
        """
        print(f"ğŸ”¥ å¼€å§‹QATè®­ç»ƒï¼Œè®­ç»ƒè½®æ•°: {num_epochs}")
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  è®­ç»ƒè®¾å¤‡: {device}")
        
        # å°†æ¨¡å‹ç§»åˆ°GPU
        model = model.to(device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        if hasattr(self.cfg.model, 'cls_args'):
            # åˆ†ç±»ä»»åŠ¡
            criterion = nn.CrossEntropyLoss().to(device)
            task_type = 'classification'
        else:
            # åˆ†å‰²ä»»åŠ¡
            criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)
            task_type = 'segmentation'
        
        model.train()
        
        for epoch in range(num_epochs):
            total_loss = 0.0
            num_batches = 0
            
            print(f"ğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
            
            for i, data in enumerate(train_loader):
                if i >= 20:  # é™åˆ¶æ¯ä¸ªepochçš„batchæ•°é‡ï¼Œç”¨äºå¿«é€ŸéªŒè¯
                    break
                
                try:
                    optimizer.zero_grad()
                    
                    # å¤„ç†è¾“å…¥æ•°æ®å¹¶ç§»åˆ°GPU
                    if isinstance(data, dict):
                        inputs = {k: v.to(device) if torch.is_tensor(v) else v for k, v in data.items()}
                        if task_type == 'classification':
                            targets = inputs.get('y', torch.randint(0, 40, (inputs['pos'].shape[0],))).to(device)
                        else:
                            targets = inputs.get('y', torch.randint(0, 13, inputs['pos'].shape[:2])).to(device)
                    else:
                        inputs = data[0] if isinstance(data, (list, tuple)) else data
                        if torch.is_tensor(inputs):
                            inputs = inputs.to(device)
                        elif isinstance(inputs, dict):
                            inputs = {k: v.to(device) if torch.is_tensor(v) else v for k, v in inputs.items()}
                        
                        if task_type == 'classification':
                            targets = torch.randint(0, 40, (inputs['pos'].shape[0] if isinstance(inputs, dict) else inputs.shape[0],)).to(device)
                        else:
                            targets = torch.randint(0, 13, inputs['pos'].shape[:2] if isinstance(inputs, dict) else inputs.shape[:2]).to(device)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    if task_type == 'classification':
                        loss = criterion(outputs, targets.long())
                    else:
                        # åˆ†å‰²ä»»åŠ¡éœ€è¦reshape
                        outputs = outputs.view(-1, outputs.shape[-1])
                        targets = targets.view(-1)
                        loss = criterion(outputs, targets.long())
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    if i % 10 == 0:
                        print(f"  Batch {i}, Loss: {loss.item():.4f}")
                        
                except Exception as e:
                    print(f"âš ï¸  è®­ç»ƒæ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue
            
            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            print(f"âœ… Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        print("âœ… QATè®­ç»ƒå®Œæˆ")
        return model

    def qat_version_model(self, model, train_loader):
        """
        FX QATé‡åŒ–æµç¨‹ - ä¸“æ³¨è°ƒè¯•
        """
        print("ğŸ”¥ å¼€å§‹FX QATé‡åŒ–å‡†å¤‡...")
        
        # æ£€æŸ¥FX QATæ”¯æŒ
        if prepare_qat_fx is None:
            raise RuntimeError("å½“å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒFX QAT")
        
        try:
            # 1. å¤åˆ¶æ¨¡å‹å¹¶å‡†å¤‡QAT
            model_to_quantize = copy.deepcopy(model)
            model_to_quantize.eval()  # QATå‡†å¤‡æ—¶éœ€è¦evalæ¨¡å¼
            
            # 2. åº”ç”¨FXå…¼å®¹è¡¥ä¸
            restore_patches = None
            if FX_PATCH_AVAILABLE:
                restore_patches = apply_fx_patches()
            
            # 3. åˆ›å»ºç¤ºä¾‹è¾“å…¥
            example_inputs = self._create_example_input()
            print(f"ğŸ“ ç¤ºä¾‹è¾“å…¥å½¢çŠ¶: {example_inputs['pos'].shape}")
            
            # 4. å°è¯•ç¬¦å·åŒ–è¿½è¸ªï¼Œæ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            print("ğŸ” å¼€å§‹ç¬¦å·åŒ–è¿½è¸ª...")
            try:
                # å…ˆåˆ†ææ¨¡å‹ç»“æ„
                self._analyze_model_structure(model_to_quantize)
                
                traced_model = symbolic_trace(model_to_quantize)
                print("âœ… ç¬¦å·åŒ–è¿½è¸ªæˆåŠŸ")
                print(f"ğŸ“Š è¿½è¸ªå›¾èŠ‚ç‚¹æ•°: {len(traced_model.graph.nodes)}")
            except Exception as trace_error:
                print(f"âŒ ç¬¦å·åŒ–è¿½è¸ªå¤±è´¥: {trace_error}")
                print("ğŸ” å°è¯•åˆ†æå¤±è´¥åŸå› ...")
                
                # è¯¦ç»†åˆ†æå¤±è´¥åŸå› 
                self._debug_trace_failure(model_to_quantize, example_inputs)
                raise trace_error
            
            # 4. å‡†å¤‡QATé…ç½®
            qconfig_dict = {
                "": torch.quantization.get_default_qat_qconfig('fbgemm'),
            }
            
            # æ·»åŠ object_typeé…ç½®æ¥è·³è¿‡ç‰¹å®šç±»å‹çš„å±‚
            if FX_LAYER_IMPORTS_AVAILABLE:
                qconfig_dict["object_type"] = [
                    (FurthestPointSampling, None),  # è·³è¿‡FPSå±‚
                    (QueryAndGroup, None),          # è·³è¿‡æŸ¥è¯¢å’Œåˆ†ç»„å±‚
                    (BallQuery, None),              # è·³è¿‡çƒæŸ¥è¯¢å±‚
                    (GroupingOperation, None),      # è·³è¿‡åˆ†ç»„æ“ä½œå±‚
                ]
                print("ğŸš« é…ç½®è·³è¿‡çš„å±‚ç±»å‹:")
                print("  - FurthestPointSampling (æœ€è¿œç‚¹é‡‡æ ·)")
                print("  - QueryAndGroup (æŸ¥è¯¢å’Œåˆ†ç»„)")
                print("  - BallQuery (çƒæŸ¥è¯¢)")
                print("  - GroupingOperation (åˆ†ç»„æ“ä½œ)")
            else:
                print("âš ï¸ æ— æ³•ä½¿ç”¨object_typeé…ç½®ï¼Œå°†åœ¨åç»­ä½¿ç”¨æ¨¡å—ååŒ¹é…")
            
            # æ·»åŠ åŸºäºæ¨¡å—åçš„è·³è¿‡é…ç½®ï¼ˆæ›´ç²¾ç¡®çš„æ§åˆ¶ï¼‰
            # éå†æ¨¡å‹æ‰¾åˆ°éœ€è¦è·³è¿‡çš„å…·ä½“æ¨¡å—
            skip_module_names = []
            for name, module in model_to_quantize.named_modules():
                module_type = type(module).__name__
                if any(pattern in module_type for pattern in [
                    'FurthestPointSampling', 'QueryAndGroup', 'BallQuery', 
                    'GroupingOperation', 'GroupAll', 'KNNGroup'
                ]):
                    skip_module_names.append(name)
                    qconfig_dict[name] = None  # è·³è¿‡è¿™ä¸ªå…·ä½“æ¨¡å—
            
            if skip_module_names:
                print("ğŸš« åŸºäºæ¨¡å—åè·³è¿‡çš„å±‚:")
                for name in skip_module_names[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  - {name}")
                if len(skip_module_names) > 5:
                    print(f"  - ... ä»¥åŠå…¶ä»– {len(skip_module_names) - 5} ä¸ªæ¨¡å—")
            
            print(f"ğŸ“Š é‡åŒ–é…ç½®ç»Ÿè®¡: è·³è¿‡ {len(skip_module_names)} ä¸ªæ¨¡å—")
            
            # 5. å‡†å¤‡QATæ¨¡å‹
            print("ğŸ”§ å‡†å¤‡QATæ¨¡å‹...")
            model_prepared = prepare_qat_fx(traced_model, qconfig_dict, example_inputs)
            print("âœ… QATæ¨¡å‹å‡†å¤‡å®Œæˆ")
            
            # 6. QATè®­ç»ƒ
            model_trained = self.qat_train_model(model_prepared, train_loader, num_epochs=3)
            
            # 7. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
            print("ğŸ”„ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
            model_trained.eval()  # è½¬æ¢å‰å¿…é¡»è®¾ç½®ä¸ºevalæ¨¡å¼
            quantized_model = convert_fx(model_trained)
            print("âœ… FX QATé‡åŒ–è½¬æ¢å®Œæˆ")
            
            return quantized_model
            
        except Exception as e:
            print(f"âŒ FX QATé‡åŒ–å¤±è´¥: {e}")
            print("ï¿½ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            import traceback
            traceback.print_exc()
            raise e

    def _legacy_qat_quantize(self, model, train_loader):
        """
        ä¼ ç»ŸQATé‡åŒ–æ–¹æ³• - å…¼å®¹æ—§ç‰ˆæœ¬PyTorch
        """
        print("ğŸ”§ ä½¿ç”¨ä¼ ç»ŸQATé‡åŒ–æ–¹æ³•...")
        
        try:
            # 1. å¤åˆ¶æ¨¡å‹å¹¶ç§»åˆ°CPUè¿›è¡Œé‡åŒ–
            model_to_quantize = copy.deepcopy(model)
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"âš ï¸  æ³¨æ„ï¼šé‡åŒ–æ“ä½œéœ€è¦åœ¨CPUä¸Šè¿›è¡Œï¼Œè®­ç»ƒåœ¨{device}ä¸Šè¿›è¡Œ")
            
            # å…ˆåœ¨GPUä¸Šè®­ç»ƒï¼Œç„¶åç§»åˆ°CPUé‡åŒ–
            model_to_quantize = model_to_quantize.to(device)
            
            # 2. è®¾ç½®QATé…ç½®
            model_to_quantize.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
            
            # 3. å‡†å¤‡QAT
            model_prepared = torch.quantization.prepare_qat(model_to_quantize)
            print("âœ… ä¼ ç»ŸQATæ¨¡å‹å‡†å¤‡å®Œæˆ")
            
            # 4. QATè®­ç»ƒï¼ˆåœ¨GPUä¸Šï¼‰
            model_trained = self.qat_train_model(model_prepared, train_loader, num_epochs=3)
            
            # 5. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹ï¼ˆç§»åˆ°CPUè¿›è¡Œï¼‰
            model_trained.eval()
            model_trained = model_trained.cpu()  # ç§»åˆ°CPUè¿›è¡Œé‡åŒ–è½¬æ¢
            print("ğŸ”„ å°†æ¨¡å‹ç§»è‡³CPUè¿›è¡Œé‡åŒ–è½¬æ¢...")
            
            quantized_model = torch.quantization.convert(model_trained)
            print("âœ… ä¼ ç»ŸQATé‡åŒ–è½¬æ¢å®Œæˆ")
            
            return quantized_model
            
        except Exception as e:
            print(f"âŒ ä¼ ç»ŸQATé‡åŒ–ä¹Ÿå¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°é™æ€é‡åŒ–...")
            # ç¡®ä¿æ¨¡å‹åœ¨CPUä¸Šè¿›è¡Œé™æ€é‡åŒ–
            model_cpu = model.cpu()
            return self.quantize_model(model_cpu, train_loader)

    def _analyze_model_structure(self, model):
        """åˆ†ææ¨¡å‹ç»“æ„ä»¥æ‰¾å‡ºFXè¿½è¸ªå¤±è´¥çš„åŸå› """
        print("ğŸ” æ¨¡å‹ç»“æ„åˆ†æ:")
        
        # 1. æ£€æŸ¥æ¨¡å‹å±‚çº§
        print("ğŸ“‹ æ¨¡å‹å±‚çº§ç»“æ„:")
        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # å¶å­èŠ‚ç‚¹
                print(f"  {name}: {type(module).__name__}")
        
        # 2. æ£€æŸ¥å‰å‘ä¼ æ’­ä¸­çš„é—®é¢˜èŠ‚ç‚¹
        print("\nğŸ” æ£€æŸ¥é—®é¢˜æ“ä½œ:")
        problematic_ops = []
        
        for name, module in model.named_modules():
            module_type = type(module).__name__
            if any(op in module_type.lower() for op in ['assert', 'conditional', 'if', 'while']):
                problematic_ops.append((name, module_type))
        
        if problematic_ops:
            print("âš ï¸  å‘ç°å¯èƒ½å¯¼è‡´è¿½è¸ªå¤±è´¥çš„æ“ä½œ:")
            for name, op_type in problematic_ops:
                print(f"    {name}: {op_type}")
        
        # 3. å°è¯•å•æ­¥å‰å‘ä¼ æ’­
        print("\nğŸ” å°è¯•å•æ­¥å‰å‘ä¼ æ’­è°ƒè¯•:")
        try:
            example_input = self._create_example_input()
            with torch.no_grad():
                # è®¾ç½®hookæ¥æ•è·æ¯å±‚çš„è¾“å‡º
                def debug_hook(name):
                    def hook_fn(module, input, output):
                        print(f"  âœ… {name}: {type(module).__name__} -> {type(output)}")
                        if hasattr(output, 'shape'):
                            print(f"     å½¢çŠ¶: {output.shape}")
                        elif isinstance(output, (list, tuple)):
                            print(f"     è¾“å‡ºç±»å‹: {type(output)}, é•¿åº¦: {len(output)}")
                    return hook_fn
                
                # æ³¨å†Œhooks
                hooks = []
                for name, module in model.named_modules():
                    if len(list(module.children())) == 0:  # åªåœ¨å¶å­èŠ‚ç‚¹æ³¨å†Œ
                        hook = module.register_forward_hook(debug_hook(name))
                        hooks.append(hook)
                
                # æ‰§è¡Œå‰å‘ä¼ æ’­
                output = model(example_input)
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
                
                # æ¸…ç†hooks
                for hook in hooks:
                    hook.remove()
                    
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _debug_trace_failure(self, model, example_input):
        """è°ƒè¯•è¿½è¸ªå¤±è´¥çš„å…·ä½“åŸå› """
        print("ğŸ” å¼€å§‹è¿½è¸ªå¤±è´¥è°ƒè¯•...")
        
        # 1. å°è¯•é€å±‚è¿½è¸ª
        print("ğŸ“‹ å°è¯•é€å±‚è¿½è¸ª:")
        modules = list(model.named_modules())
        
        for i, (name, module) in enumerate(modules[:10]):  # åªæ£€æŸ¥å‰10å±‚
            if len(list(module.children())) == 0:  # å¶å­èŠ‚ç‚¹
                try:
                    print(f"  æµ‹è¯• {name}: {type(module).__name__}")
                    traced_module = torch.fx.symbolic_trace(module)
                    print(f"    âœ… å¯è¿½è¸ª")
                except Exception as e:
                    print(f"    âŒ ä¸å¯è¿½è¸ª: {e}")
        
        # 2. æ£€æŸ¥æ¨¡å‹ä¸­çš„æ§åˆ¶æµ
        print("\nğŸ” æ£€æŸ¥æ§åˆ¶æµ:")
        model_code = str(model.__class__)
        print(f"æ¨¡å‹ç±»: {model_code}")
        
        # 3. å°è¯•éƒ¨åˆ†è¿½è¸ª
        print("\nğŸ” å°è¯•éƒ¨åˆ†è¿½è¸ª:")
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰forwardæ–¹æ³•
            forward_method = getattr(model, 'forward', None)
            if forward_method:
                import inspect
                source = inspect.getsource(forward_method)
                print("Forwardæ–¹æ³•æºç ç‰‡æ®µ:")
                lines = source.split('\n')[:10]  # å‰10è¡Œ
                for line in lines:
                    print(f"  {line}")
        except Exception as e:
            print(f"æ— æ³•è·å–æºç : {e}")
    
    def _create_example_input(self):
        """åˆ›å»ºç¤ºä¾‹è¾“å…¥ç”¨äºæ¨¡å‹è¿½è¸ª"""
        batch_size = 1
        num_points = 1024
        
        # PointNeXtæ­£ç¡®çš„è¾“å…¥æ ¼å¼
        pos = torch.randn(batch_size, num_points, 3)  # (B, N, 3)
        
        return {
            'pos': pos
        }
        

    def prepare_data(self) -> torch.utils.data.DataLoader:
        """å‡†å¤‡æ ¡å‡†æ•°æ®"""
        print("ğŸ“Š å‡†å¤‡æ ¡å‡†æ•°æ®...")
        
        # æ„å»ºæ•°æ®åŠ è½½å™¨
        try:
            # ä¿®æ”¹é…ç½®ä»¥è·å–å°æ‰¹é‡æ•°æ®ç”¨äºæ ¡å‡†
            cal_cfg = self.cfg.copy()
            cal_cfg.dataset.common.train.batch_size = 8  # å°æ‰¹é‡
            cal_cfg.dataset.common.train.num_workers = 2
            
            # æ„å»ºæ ¡å‡†æ•°æ®åŠ è½½å™¨
            dataloader = build_dataloader_from_cfg(cal_cfg.get('dataset', {}))
            cal_loader = dataloader['train'] if 'train' in dataloader else dataloader
            
            print(f"âœ… æ ¡å‡†æ•°æ®å‡†å¤‡å®Œæˆ")
            return cal_loader
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ ¡å‡†...")
            return self._create_synthetic_data()
    
    def _create_synthetic_data(self):
        """åˆ›å»ºåˆæˆæ•°æ®ç”¨äºæ ¡å‡†"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        class SyntheticDataset:
            def __init__(self, num_samples=100):
                self.num_samples = num_samples
                self.device = device
                
            def __len__(self):
                return self.num_samples
                
            def __getitem__(self, idx):
                # åˆ›å»ºåˆæˆç‚¹äº‘æ•°æ® - æ­£ç¡®çš„PointNeXtè¾“å…¥æ ¼å¼
                num_points = 1024
                # PointNeXtæœŸæœ›çš„è¾“å…¥æ ¼å¼: (num_points, 3) for pos
                # å§‹ç»ˆåœ¨CPUä¸Šåˆ›å»ºæ•°æ®ï¼Œé¿å…è®¾å¤‡å†²çª
                pos = torch.randn(num_points, 3)  
                
                # æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›ä¸åŒæ ¼å¼
                data = {
                    'pos': pos,
                    'y': torch.randint(0, 40, ())  # æ ‡é‡å½¢å¼
                }
                return data
        
        dataset = SyntheticDataset()
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False)
        return dataloader
    
    def quantize_model(self, model: nn.Module, calibration_loader) -> nn.Module:
        """
        ä½¿ç”¨PyTorch FXè¿›è¡Œæ¨¡å‹é‡åŒ– - å…¼å®¹ä¸åŒç‰ˆæœ¬
        """
        print("ğŸ”¥ å¼€å§‹æ¨¡å‹é‡åŒ–...")
        
        # è®¾ç½®é‡åŒ–é…ç½®
        model.eval()
        
        # 1. å°è¯•FXé‡åŒ–
        try:
            return self._fx_quantize(model, calibration_loader)
        except Exception as e:
            print(f"âŒ FXé‡åŒ–å¤±è´¥: {e}")
            print("ï¿½ å°è¯•ä¼ ç»Ÿé‡åŒ–æ–¹æ³•...")
            return self._manual_quantize(model, calibration_loader)
    
    def _fx_quantize(self, model: nn.Module, calibration_loader) -> nn.Module:
        """FXé‡åŒ–æ–¹æ³•"""
        print("ğŸ“ˆ å°è¯•FXé‡åŒ–...")
        
        # è·å–ç¤ºä¾‹è¾“å…¥
        sample_data = next(iter(calibration_loader))
        if isinstance(sample_data, dict):
            example_inputs = sample_data
        else:
            example_inputs = sample_data[0] if isinstance(sample_data, (list, tuple)) else sample_data
        
        # ç¡®ä¿è¾“å…¥åœ¨CPUä¸Š
        if isinstance(example_inputs, dict):
            example_inputs = {k: v.cpu() if torch.is_tensor(v) else v 
                            for k, v in example_inputs.items()}
        
        # ç¬¦å·åŒ–è¿½è¸ª
        traced_model = symbolic_trace(model)
        print("âœ… ç¬¦å·åŒ–è¿½è¸ªæˆåŠŸ")
        
        # å‡†å¤‡é‡åŒ– - å…¼å®¹ä¸åŒç‰ˆæœ¬
        try:
            # æ–°ç‰ˆæœ¬API
            qconfig_mapping = self.qconfig_mapping
            if callable(qconfig_mapping):
                # å¦‚æœæ˜¯å‡½æ•°ï¼Œè¯´æ˜æ˜¯å…¼å®¹æ€§åŒ…è£…
                qconfig_dict = {"": qconfig_mapping}
            else:
                qconfig_dict = qconfig_mapping
                
            prepared_model = prepare_fx(traced_model, qconfig_dict, example_inputs)
        except Exception as e:
            print(f"æ–°ç‰ˆFX APIå¤±è´¥: {e}, å°è¯•æ—§ç‰ˆAPI...")
            # æ—§ç‰ˆæœ¬API
            qconfig_dict = {"": torch.quantization.get_default_qconfig('fbgemm')}
            prepared_model = prepare_fx(traced_model, qconfig_dict, example_inputs)
        
        print("âœ… é‡åŒ–å‡†å¤‡å®Œæˆ")
        
        # æ ¡å‡†
        print("ğŸ“Š å¼€å§‹æ ¡å‡†...")
        prepared_model.eval()
        
        with torch.no_grad():
            for i, data in enumerate(calibration_loader):
                if i >= 10:  # åªä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œæ ¡å‡†
                    break
                    
                try:
                    if isinstance(data, dict):
                        inputs = data
                    else:
                        inputs = data[0] if isinstance(data, (list, tuple)) else data
                    
                    # ç¡®ä¿è¾“å…¥åœ¨CPUä¸Š
                    if isinstance(inputs, dict):
                        inputs = {k: v.cpu() if torch.is_tensor(v) else v 
                                for k, v in inputs.items()}
                    
                    _ = prepared_model(inputs)
                    
                except Exception as e:
                    print(f"âš ï¸  æ ¡å‡†æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue
        
        print("âœ… æ ¡å‡†å®Œæˆ")
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        print("ğŸ”„ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
        quantized_model = convert_fx(prepared_model)
        print("âœ… FXé‡åŒ–è½¬æ¢æˆåŠŸ")
        return quantized_model
    
    def _manual_quantize(self, model: nn.Module, calibration_loader) -> nn.Module:
        """
        æ‰‹åŠ¨é‡åŒ–æ–¹æ³•ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        """
        print("ğŸ”§ ä½¿ç”¨æ‰‹åŠ¨é‡åŒ–æ–¹æ³•...")
        
        # ç¡®ä¿æ¨¡å‹åœ¨CPUä¸Šè¿›è¡Œé‡åŒ–
        model = model.cpu()
        print("ğŸ”„ å°†æ¨¡å‹ç§»è‡³CPUè¿›è¡Œé‡åŒ–...")
        
        # è®¾ç½®é‡åŒ–é…ç½®
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # å‡†å¤‡é‡åŒ–
        torch.quantization.prepare(model, inplace=True)
        
        # æ ¡å‡†
        model.eval()
        with torch.no_grad():
            for i, data in enumerate(calibration_loader):
                if i >= 10:  # å°‘é‡æ ¡å‡†æ•°æ®
                    break
                try:
                    if isinstance(data, dict):
                        # å°†æ•°æ®ç§»åˆ°CPU
                        inputs = {k: v.cpu() if torch.is_tensor(v) else v for k, v in data.items()}
                    else:
                        inputs = data[0] if isinstance(data, (list, tuple)) else data
                        if torch.is_tensor(inputs):
                            inputs = inputs.cpu()
                        elif isinstance(inputs, dict):
                            inputs = {k: v.cpu() if torch.is_tensor(v) else v for k, v in inputs.items()}
                    
                    _ = model(inputs)
                except:
                    continue
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        torch.quantization.convert(model, inplace=True)
        
        print("âœ… æ‰‹åŠ¨é‡åŒ–å®Œæˆ")
        return model
    
    def evaluate_model(self, model: nn.Module, test_loader, model_name: str = "æ¨¡å‹"):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼°{model_name}æ€§èƒ½...")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ£€æµ‹æ˜¯å¦ä¸ºé‡åŒ–æ¨¡å‹
        is_quantized = any(hasattr(m, '_weight_bias') or 'quantized' in str(type(m)).lower() 
                          for m in model.modules())
        
        if is_quantized:
            # é‡åŒ–æ¨¡å‹åªèƒ½åœ¨CPUä¸Šè¿è¡Œ
            print("ğŸ”„ æ£€æµ‹åˆ°é‡åŒ–æ¨¡å‹ï¼Œå°†åœ¨CPUä¸Šè¯„ä¼°...")
            model = model.cpu()
            eval_device = torch.device('cpu')
        else:
            # åŸå§‹æ¨¡å‹å¯ä»¥åœ¨GPUä¸Šè¿è¡Œ
            model = model.to(device)
            eval_device = device
            
        model.eval()
        total_time = 0
        num_batches = 0
        
        with torch.no_grad():
            for i, data in enumerate(test_loader):
                if i >= 20:  # åªæµ‹è¯•å°‘é‡æ‰¹æ¬¡
                    break
                
                try:
                    if isinstance(data, dict):
                        inputs = {k: v.to(eval_device) if torch.is_tensor(v) else v for k, v in data.items()}
                    else:
                        inputs = data[0] if isinstance(data, (list, tuple)) else data
                        if torch.is_tensor(inputs):
                            inputs = inputs.to(eval_device)
                        elif isinstance(inputs, dict):
                            inputs = {k: v.to(eval_device) if torch.is_tensor(v) else v for k, v in inputs.items()}
                    
                    # è®¡æ—¶
                    if eval_device.type == 'cuda':
                        start_time = torch.cuda.Event(enable_timing=True)
                        end_time = torch.cuda.Event(enable_timing=True)
                        start_time.record()
                    else:
                        import time
                        start = time.time()
                    
                    _ = model(inputs)
                    
                    if eval_device.type == 'cuda':
                        end_time.record()
                        torch.cuda.synchronize()
                        batch_time = start_time.elapsed_time(end_time)
                    else:
                        batch_time = (time.time() - start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    
                    total_time += batch_time
                    num_batches += 1
                    
                except Exception as e:
                    print(f"âš ï¸  è¯„ä¼°æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue
        
        avg_time = total_time / num_batches if num_batches > 0 else 0
        print(f"ğŸ“ˆ {model_name}å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms (è®¾å¤‡: {eval_device})")
        return avg_time
    
    def compare_models(self, original_model: nn.Module, quantized_model: nn.Module, 
                      test_loader):
        """æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹"""
        print("\n" + "="*50)
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print("="*50)
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹
        original_time = self.evaluate_model(original_model, test_loader, "åŸå§‹æ¨¡å‹")
        
        # è¯„ä¼°é‡åŒ–æ¨¡å‹
        quantized_time = self.evaluate_model(quantized_model, test_loader, "é‡åŒ–æ¨¡å‹")
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        def get_model_size(model):
            total_params = sum(p.numel() * p.element_size() for p in model.parameters())
            total_buffers = sum(b.numel() * b.element_size() for b in model.buffers())
            return (total_params + total_buffers) / 1024 / 1024  # MB
        
        original_size = get_model_size(original_model)
        quantized_size = get_model_size(quantized_model)
        
        # è¾“å‡ºå¯¹æ¯”ç»“æœ
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        print(f"  åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´: {original_time:.2f} ms")
        print(f"  é‡åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {quantized_time:.2f} ms")
        print(f"  é€Ÿåº¦æå‡: {original_time/quantized_time:.2f}x" if quantized_time > 0 else "  é€Ÿåº¦æå‡: N/A")
        
        print(f"\nğŸ’¾ æ¨¡å‹å¤§å°å¯¹æ¯”:")
        print(f"  åŸå§‹æ¨¡å‹å¤§å°: {original_size:.2f} MB")
        print(f"  é‡åŒ–æ¨¡å‹å¤§å°: {quantized_size:.2f} MB")
        print(f"  å¤§å°å‹ç¼©: {original_size/quantized_size:.2f}x" if quantized_size > 0 else "  å¤§å°å‹ç¼©: N/A")
        
        return {
            'original_time': original_time,
            'quantized_time': quantized_time,
            'original_size': original_size,
            'quantized_size': quantized_size,
            'speed_up': original_time/quantized_time if quantized_time > 0 else 0,
            'compression': original_size/quantized_size if quantized_size > 0 else 0
        }
    
    def save_quantized_model(self, quantized_model: nn.Module, save_path: str):
        """ä¿å­˜é‡åŒ–æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹åˆ°: {save_path}")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        try:
            # å°è¯•ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŒ…æ‹¬é‡åŒ–ä¿¡æ¯ï¼‰
            torch.save({
                'model_state_dict': quantized_model.state_dict(),
                'quantization_info': 'QAT quantized model',
                'model_class': type(quantized_model).__name__
            }, save_path)
            print("âœ… é‡åŒ–æ¨¡å‹ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å®Œæ•´æ¨¡å‹å¤±è´¥: {e}")
            try:
                # åªä¿å­˜çŠ¶æ€å­—å…¸
                torch.save(quantized_model.state_dict(), save_path.replace('.pth', '_state_dict.pth'))
                print(f"ğŸ’¾ æ¨¡å‹çŠ¶æ€å­—å…¸å·²ä¿å­˜åˆ°: {save_path.replace('.pth', '_state_dict.pth')}")
                print("âœ… é‡åŒ–æ¨¡å‹çŠ¶æ€å­—å…¸ä¿å­˜æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e2}")
    
    def run_quantization(self, save_path: str = None):
        """è¿è¡Œå®Œæ•´çš„é‡åŒ–æµç¨‹"""
        print("\n" + "ğŸš€ å¼€å§‹PointNeXté‡åŒ–æµç¨‹" + "\n" + "="*50)
        
        # 1. æ„å»ºæ¨¡å‹
        original_model = self.build_model()
        
        # 2. å‡†å¤‡æ•°æ®
        calibration_loader = self.prepare_data()
        
        # 3. é‡åŒ–æ¨¡å‹
        quantized_model = self.quantize_model(original_model.cpu(), calibration_loader)
        
        # 4. æ€§èƒ½å¯¹æ¯”
        results = self.compare_models(original_model, quantized_model, calibration_loader)
        
        # 5. ä¿å­˜æ¨¡å‹
        if save_path:
            self.save_quantized_model(quantized_model, save_path)
        
        print("\n" + "ğŸ‰ é‡åŒ–æµç¨‹å®Œæˆ!" + "\n" + "="*50)
        
        return quantized_model, results

    def run_qat_quantization(self, save_path: str = None, num_epochs: int = 3):
        """è¿è¡Œå®Œæ•´çš„QATé‡åŒ–æµç¨‹"""
        print("\n" + "ğŸš€ å¼€å§‹PointNeXt QATé‡åŒ–æµç¨‹" + "\n" + "="*50)
        
        # 1. æ„å»ºæ¨¡å‹
        original_model = self.build_model()
        
        # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
        train_loader = self.prepare_data()
        
        # 3. è¿è¡ŒQATé‡åŒ–ï¼ˆåŒ…å«è®­ç»ƒè¿‡ç¨‹ï¼‰
        print("ğŸ”¥ å¼€å§‹QATé‡åŒ–è®­ç»ƒ...")
        quantized_model = self.qat_version_model(original_model, train_loader)
        
        # 4. æ€§èƒ½å¯¹æ¯”
        results = self.compare_models(original_model, quantized_model, train_loader)
        
        # 5. ä¿å­˜æ¨¡å‹
        if save_path:
            # ä¿®æ”¹ä¿å­˜è·¯å¾„ä»¥åŒºåˆ†QATæ¨¡å‹
            qat_save_path = save_path.replace('.pth', '_qat.pth')
            self.save_quantized_model(quantized_model, qat_save_path)
            print(f"ğŸ’¾ QATé‡åŒ–æ¨¡å‹ä¿å­˜åˆ°: {qat_save_path}")
        
        print("\n" + "ğŸ‰ QATé‡åŒ–æµç¨‹å®Œæˆ!" + "\n" + "="*50)
        
        return quantized_model, results

    def compare_quantization_methods(self, save_path: str = None):
        """å¯¹æ¯”é™æ€é‡åŒ–å’ŒQATé‡åŒ–çš„æ•ˆæœ"""
        print("\n" + "ğŸ”¬ å¼€å§‹é‡åŒ–æ–¹æ³•å¯¹æ¯”" + "\n" + "="*50)
        
        # 1. æ„å»ºåŸå§‹æ¨¡å‹
        original_model = self.build_model()
        data_loader = self.prepare_data()
        
        # 2. é™æ€é‡åŒ–
        print("\nğŸ“Š è¿è¡Œé™æ€é‡åŒ–...")
        static_quantized = self.quantize_model(original_model.cpu(), data_loader)
        static_results = self.compare_models(original_model, static_quantized, data_loader)
        
        # 3. QATé‡åŒ–
        print("\nğŸ¯ è¿è¡ŒQATé‡åŒ–...")
        qat_quantized = self.qat_version_model(copy.deepcopy(original_model), data_loader)
        qat_results = self.compare_models(original_model, qat_quantized, data_loader)
        
        # 4. å¯¹æ¯”ç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š é‡åŒ–æ–¹æ³•å¯¹æ¯”ç»“æœ")
        print("="*60)
        
        print(f"ğŸ”¹ åŸå§‹æ¨¡å‹:")
        print(f"  æ¨ç†æ—¶é—´: {static_results['original_time']:.2f} ms")
        print(f"  æ¨¡å‹å¤§å°: {static_results['original_size']:.2f} MB")
        
        print(f"\nğŸ”¹ é™æ€é‡åŒ–:")
        print(f"  æ¨ç†æ—¶é—´: {static_results['quantized_time']:.2f} ms")
        print(f"  æ¨¡å‹å¤§å°: {static_results['quantized_size']:.2f} MB")
        print(f"  é€Ÿåº¦æå‡: {static_results['speed_up']:.2f}x")
        print(f"  å¤§å°å‹ç¼©: {static_results['compression']:.2f}x")
        
        print(f"\nğŸ”¹ QATé‡åŒ–:")
        print(f"  æ¨ç†æ—¶é—´: {qat_results['quantized_time']:.2f} ms")
        print(f"  æ¨¡å‹å¤§å°: {qat_results['quantized_size']:.2f} MB")
        print(f"  é€Ÿåº¦æå‡: {qat_results['speed_up']:.2f}x")
        print(f"  å¤§å°å‹ç¼©: {qat_results['compression']:.2f}x")
        
        # å¯¹æ¯”é™æ€é‡åŒ–å’ŒQAT
        speed_diff = qat_results['speed_up'] / static_results['speed_up']
        size_diff = qat_results['compression'] / static_results['compression']
        
        print(f"\nğŸ”¸ QAT vs é™æ€é‡åŒ–:")
        print(f"  é€Ÿåº¦å¯¹æ¯”: {speed_diff:.2f}x {'(QATæ›´å¿«)' if speed_diff > 1 else '(é™æ€æ›´å¿«)'}")
        print(f"  å‹ç¼©å¯¹æ¯”: {size_diff:.2f}x {'(QATå‹ç¼©æ›´å¥½)' if size_diff > 1 else '(é™æ€å‹ç¼©æ›´å¥½)'}")
        
        # 5. ä¿å­˜ä¸¤ä¸ªæ¨¡å‹
        if save_path:
            static_path = save_path.replace('.pth', '_static.pth')
            qat_path = save_path.replace('.pth', '_qat.pth')
            
            self.save_quantized_model(static_quantized, static_path)
            self.save_quantized_model(qat_quantized, qat_path)
            
            print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜:")
            print(f"  é™æ€é‡åŒ–: {static_path}")
            print(f"  QATé‡åŒ–: {qat_path}")
        
        return {
            'static': static_results,
            'qat': qat_results,
            'comparison': {
                'speed_ratio': speed_diff,
                'compression_ratio': size_diff
            }
        }


def main():
    parser = argparse.ArgumentParser(description='PointNeXt PyTorch FX é‡åŒ–')
    parser.add_argument('--cfg', type=str, required=True, 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--pretrained', type=str, default=None,
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_path', type=str, default='quantized_models/quantized_model.pth',
                       help='é‡åŒ–æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è¿è¡Œè®¾å¤‡ (cpu/cuda)')
    parser.add_argument('--method', type=str, default='static', 
                       choices=['static', 'qat', 'compare'],
                       help='é‡åŒ–æ–¹æ³•: static(é™æ€é‡åŒ–), qat(QATé‡åŒ–), compare(å¯¹æ¯”ä¸¤ç§æ–¹æ³•)')
    parser.add_argument('--epochs', type=int, default=3,
                       help='QATè®­ç»ƒè½®æ•°')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.cfg):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.cfg}")
        return
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹
    if args.pretrained and not os.path.exists(args.pretrained):
        print(f"âš ï¸  é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {args.pretrained}")
        print("ğŸ”„ å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œé‡åŒ–")
        args.pretrained = None
    
    # è®¾ç½®è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = 'cpu'
    elif args.device == 'cpu' and torch.cuda.is_available():
        print("ğŸ’¡ æ£€æµ‹åˆ°CUDAå¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨ --device cuda ä»¥è·å¾—æ›´å¥½æ€§èƒ½")
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {args.device}")
    print(f"âš™ï¸  é‡åŒ–æ–¹æ³•: {args.method}")
    
    # è®¾ç½®é»˜è®¤è®¾å¤‡
    if args.device == 'cuda':
        torch.cuda.set_device(0)  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    
    # å¼€å§‹é‡åŒ–
    try:
        quantizer = SimplePointNeXtQuantizer(args.cfg, args.pretrained)
        
        if args.method == 'static':
            # é™æ€é‡åŒ–
            quantized_model, results = quantizer.run_quantization(args.save_path)
            print(f"\nğŸ¯ é™æ€é‡åŒ–æ€»ç»“:")
            print(f"  é€Ÿåº¦æå‡: {results['speed_up']:.2f}x")
            print(f"  æ¨¡å‹å‹ç¼©: {results['compression']:.2f}x")
            
        elif args.method == 'qat':
            # QATé‡åŒ–
            quantized_model, results = quantizer.run_qat_quantization(args.save_path, args.epochs)
            print(f"\nğŸ¯ QATé‡åŒ–æ€»ç»“:")
            print(f"  é€Ÿåº¦æå‡: {results['speed_up']:.2f}x")
            print(f"  æ¨¡å‹å‹ç¼©: {results['compression']:.2f}x")
            
        elif args.method == 'compare':
            # å¯¹æ¯”ä¸¤ç§æ–¹æ³•
            comparison_results = quantizer.compare_quantization_methods(args.save_path)
            print(f"\nğŸ† æœ€ä½³é‡åŒ–æ–¹æ³•æ¨è:")
            
            static_score = comparison_results['static']['speed_up'] + comparison_results['static']['compression']
            qat_score = comparison_results['qat']['speed_up'] + comparison_results['qat']['compression']
            
            if qat_score > static_score:
                print(f"  ğŸ¥‡ æ¨èQATé‡åŒ– (ç»¼åˆå¾—åˆ†: {qat_score:.2f})")
                print(f"  ğŸ¥ˆ é™æ€é‡åŒ– (ç»¼åˆå¾—åˆ†: {static_score:.2f})")
            else:
                print(f"  ğŸ¥‡ æ¨èé™æ€é‡åŒ– (ç»¼åˆå¾—åˆ†: {static_score:.2f})")
                print(f"  ğŸ¥ˆ QATé‡åŒ– (ç»¼åˆå¾—åˆ†: {qat_score:.2f})")
        
    except Exception as e:
        print(f"âŒ é‡åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
