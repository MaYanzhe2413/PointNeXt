# PointNeXt QATé‡åŒ–è¯¦ç»†æŒ‡å—

## ğŸ¯ ä»€ä¹ˆæ˜¯QATé‡åŒ–ï¼Ÿ

**QAT (Quantization Aware Training)** å³é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œæ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœçš„æŠ€æœ¯ã€‚

### ğŸ”„ QAT vs é™æ€é‡åŒ–å¯¹æ¯”

| ç‰¹æ€§ | é™æ€é‡åŒ– (PTQ) | QATé‡åŒ– |
|------|----------------|---------|
| **è®­ç»ƒéœ€æ±‚** | âŒ ä¸éœ€è¦è®­ç»ƒ | âœ… éœ€è¦è®­ç»ƒè¿‡ç¨‹ |
| **ç²¾åº¦ä¿æŒ** | âš ï¸ å¯èƒ½æœ‰ç²¾åº¦æŸå¤± | âœ… ç²¾åº¦æŸå¤±æœ€å° |
| **æ—¶é—´æˆæœ¬** | âœ… å¿«é€Ÿ (åˆ†é’Ÿçº§) | âš ï¸ è¾ƒæ…¢ (å°æ—¶çº§) |
| **é€‚ç”¨åœºæ™¯** | å·²è®­ç»ƒå®Œæˆçš„æ¨¡å‹ | å¯ä»¥é‡æ–°è®­ç»ƒçš„æ¨¡å‹ |
| **é‡åŒ–è´¨é‡** | ä¸€èˆ¬ | æœ€ä½³ |

## ğŸ”¬ QATåŸç†è¯¦è§£

### 1. **ä¼ªé‡åŒ– (Fake Quantization)**

QATçš„æ ¸å¿ƒæ˜¯åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ä¼ªé‡åŒ–ï¼š

```python
# ä¼ªé‡åŒ–è¿‡ç¨‹
def fake_quantize(x, scale, zero_point):
    # 1. é‡åŒ–åˆ°INT8
    x_quantized = torch.round(x / scale + zero_point)
    x_quantized = torch.clamp(x_quantized, 0, 255)  # 8-bitèŒƒå›´
    
    # 2. åé‡åŒ–å›FP32 (ç”¨äºæ¢¯åº¦è®¡ç®—)
    x_dequantized = (x_quantized - zero_point) * scale
    
    return x_dequantized
```

### 2. **è®­ç»ƒæµç¨‹**

```
Input (FP32) â†’ æ¨¡å‹å‰å‘ä¼ æ’­ â†’ ä¼ªé‡åŒ– â†’ æŸå¤±è®¡ç®— â†’ åå‘ä¼ æ’­ (FP32) â†’ æ›´æ–°å‚æ•°
     â†‘                              â†“
     â””â”€â”€ å‚æ•°æ›´æ–° â†â”€â”€ æ¢¯åº¦è®¡ç®— â†â”€â”€ 
```

### 3. **æˆ‘ä»¬çš„QATå®ç°**

```python
def qat_train_model(self, model, train_loader, num_epochs=3):
    # 1. è®¾ç½®QATé…ç½®
    qconfig_dict = {
        "": torch.quantization.get_default_qat_qconfig('fbgemm')
    }
    
    # 2. å‡†å¤‡QATæ¨¡å‹ (æ’å…¥ä¼ªé‡åŒ–ç®—å­)
    model_prepared = prepare_qat_fx(traced_model, qconfig_dict, example_inputs)
    
    # 3. QATè®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        for data in train_loader:
            optimizer.zero_grad()
            outputs = model_prepared(data)  # å‰å‘ä¼ æ’­å«ä¼ªé‡åŒ–
            loss = criterion(outputs, targets)
            loss.backward()  # åå‘ä¼ æ’­ç”¨FP32æ¢¯åº¦
            optimizer.step()
    
    # 4. è½¬æ¢ä¸ºçœŸå®é‡åŒ–æ¨¡å‹
    model.eval()
    quantized_model = convert_fx(model_prepared)
    return quantized_model
```

## ğŸ› ï¸ ä½¿ç”¨æˆ‘ä»¬çš„QATé‡åŒ–

### 1. **å‘½ä»¤è¡Œä½¿ç”¨**

```bash
# åŸºç¡€QATé‡åŒ–
python quantize_fx.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml --method qat

# è‡ªå®šä¹‰è®­ç»ƒè½®æ•°
python quantize_fx.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml --method qat --epochs 5

# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
python quantize_fx.py \
    --cfg cfgs/modelnet40ply2048/pointnext-s.yaml \
    --method qat \
    --pretrained /path/to/pretrained.pth \
    --epochs 3

# å¯¹æ¯”é™æ€é‡åŒ–å’ŒQAT
python quantize_fx.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml --method compare
```

### 2. **äº¤äº’å¼ä½¿ç”¨**

```bash
# å¯åŠ¨äº¤äº’å¼èœå•
./quick_quantize.sh

# é€‰æ‹©QATé€‰é¡¹ (7, 8, 9)
```

## ğŸ¯ QATä¼˜åŒ–ç­–ç•¥

### 1. **å­¦ä¹ ç‡è°ƒæ•´**

```python
# QATé€šå¸¸éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡
optimizer = optim.Adam(model.parameters(), lr=0.0001)  # æ¯”æ­£å¸¸è®­ç»ƒå°10å€
```

### 2. **è®­ç»ƒè½®æ•°é€‰æ‹©**

- **å¿«é€ŸéªŒè¯**: 1-3 epochs
- **ç”Ÿäº§ä½¿ç”¨**: 5-10 epochs  
- **ç²¾åº¦è¦æ±‚é«˜**: 10+ epochs

### 3. **é‡åŒ–é…ç½®ä¼˜åŒ–**

```python
# é’ˆå¯¹ä¸åŒå±‚ç±»å‹çš„é‡åŒ–é…ç½®
qconfig_dict = {
    "": torch.quantization.get_default_qat_qconfig('fbgemm'),
    "object_type": [
        (nn.BatchNorm1d, None),  # è·³è¿‡BNå±‚
        (nn.Dropout, None),      # è·³è¿‡Dropoutå±‚
        (nn.Softmax, special_qconfig),  # ç‰¹æ®Šé…ç½®
    ],
}
```

## ğŸ“Š é‡åŒ–æ•ˆæœåˆ†æ

### 1. **å…¸å‹QATæ•ˆæœ**

```
ğŸ“Š QATé‡åŒ–æ•ˆæœç¤ºä¾‹
==================================================
ğŸ”¹ åŸå§‹æ¨¡å‹:
  æ¨ç†æ—¶é—´: 45.32 ms
  æ¨¡å‹å¤§å°: 15.34 MB
  ç²¾åº¦: 92.3%

ğŸ”¹ é™æ€é‡åŒ–:
  æ¨ç†æ—¶é—´: 23.67 ms (1.91x)
  æ¨¡å‹å¤§å°: 4.12 MB (3.72x)
  ç²¾åº¦: 90.8% (-1.5%)

ğŸ”¹ QATé‡åŒ–:
  æ¨ç†æ—¶é—´: 22.15 ms (2.05x)
  æ¨¡å‹å¤§å°: 4.12 MB (3.72x)
  ç²¾åº¦: 91.9% (-0.4%)
```

### 2. **QAT vs é™æ€é‡åŒ–ä¼˜åŠ¿**

- **ç²¾åº¦ä¼˜åŠ¿**: é€šå¸¸æ¯”é™æ€é‡åŒ–ç²¾åº¦é«˜1-2%
- **é²æ£’æ€§**: å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–æ›´åŠ é²æ£’
- **æé™ä¼˜åŒ–**: å¯ä»¥è¾¾åˆ°æ¥è¿‘åŸå§‹æ¨¡å‹çš„ç²¾åº¦

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. **æ•°æ®å¹¶è¡ŒQAT**

```python
# å¤šGPU QATè®­ç»ƒ
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
```

### 2. **æ¸è¿›å¼é‡åŒ–**

```python
# å…ˆè®­ç»ƒå‡ ä¸ªepochä¸é‡åŒ–ï¼Œå†å¼€å¯é‡åŒ–
for epoch in range(total_epochs):
    if epoch < warmup_epochs:
        # æ­£å¸¸è®­ç»ƒ
        model.apply(torch.quantization.disable_fake_quant)
    else:
        # QATè®­ç»ƒ
        model.apply(torch.quantization.enable_fake_quant)
```

### 3. **çŸ¥è¯†è’¸é¦ + QAT**

```python
# ç»“åˆçŸ¥è¯†è’¸é¦çš„QAT
teacher_model = load_pretrained_model()
student_model = qat_prepared_model

loss = alpha * task_loss + (1-alpha) * distillation_loss
```

## ğŸš¨ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. **QATè®­ç»ƒä¸æ”¶æ•›**

**åŸå› **: å­¦ä¹ ç‡è¿‡é«˜ã€ä¼ªé‡åŒ–å™ªå£°è¿‡å¤§
**è§£å†³**: é™ä½å­¦ä¹ ç‡ã€å¢åŠ è®­ç»ƒè½®æ•°

### 2. **ç²¾åº¦ä¸‹é™ä¸¥é‡**

**åŸå› **: é‡åŒ–é…ç½®ä¸å½“ã€è®­ç»ƒæ•°æ®ä¸è¶³
**è§£å†³**: è°ƒæ•´qconfigã€å¢åŠ æ ¡å‡†æ•°æ®

### 3. **è®­ç»ƒæ—¶é—´è¿‡é•¿**

**åŸå› **: ä¼ªé‡åŒ–å¢åŠ è®¡ç®—å¼€é”€
**è§£å†³**: å‡å°‘è®­ç»ƒæ•°æ®ã€ä½¿ç”¨æ›´å°çš„æ¨¡å‹

## ğŸ† æœ€ä½³å®è·µæ¨è

### 1. **é€‰æ‹©ç­–ç•¥**

```
â”Œâ”€ æ¨¡å‹å·²è®­ç»ƒå®Œæˆï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ ç²¾åº¦è¦æ±‚é«˜ï¼Ÿ
â”‚  â”‚     â”œâ”€ æ˜¯ â†’ QATé‡åŒ–
â”‚  â”‚     â””â”€ å¦ â†’ é™æ€é‡åŒ–
â”‚  â””â”€ å¦ â†’ ç›´æ¥QATè®­ç»ƒ
```

### 2. **å·¥ç¨‹å®è·µ**

1. **åŸå‹é˜¶æ®µ**: ä½¿ç”¨é™æ€é‡åŒ–å¿«é€ŸéªŒè¯
2. **ä¼˜åŒ–é˜¶æ®µ**: ä½¿ç”¨QATæå‡ç²¾åº¦
3. **ç”Ÿäº§é˜¶æ®µ**: æ ¹æ®ç²¾åº¦è¦æ±‚é€‰æ‹©æ–¹æ¡ˆ

### 3. **è°ƒå‚å»ºè®®**

- **å­¦ä¹ ç‡**: åŸå§‹è®­ç»ƒçš„0.1å€
- **è®­ç»ƒè½®æ•°**: 3-5 epochsé€šå¸¸è¶³å¤Ÿ
- **Batch Size**: ä¿æŒä¸åŸå§‹è®­ç»ƒä¸€è‡´
- **æ•°æ®å¢å¼º**: é€‚å½“å‡å°‘ï¼Œé¿å…è¿‡åº¦æ‰°åŠ¨

---

QATé‡åŒ–æ˜¯å®ç°é«˜ç²¾åº¦é‡åŒ–çš„æœ€ä½³æ–¹æ¡ˆï¼Œè™½ç„¶éœ€è¦é¢å¤–çš„è®­ç»ƒæ—¶é—´ï¼Œä½†èƒ½å¤Ÿæ˜¾è‘—æå‡é‡åŒ–æ¨¡å‹çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬çš„å®ç°æä¾›äº†å®Œæ•´çš„QATæµç¨‹ï¼Œè®©ä½ èƒ½å¤Ÿè½»æ¾ä½¿ç”¨è¿™é¡¹å…ˆè¿›æŠ€æœ¯ã€‚
