#!/usr/bin/env python3
"""
PointNeXt Eageræ¨¡å¼é‡åŒ–è„šæœ¬ï¼ˆä»…ä½¿ç”¨çœŸå®ModelNet40æ•°æ®ï¼‰
ä½¿ç”¨PyTorch Eageræ¨¡å¼è¿›è¡Œé‡åŒ–ï¼Œå®Œå…¨å…¼å®¹CUDAæ“ä½œå’Œæ§åˆ¶æµ
"""

import os
import sys
import copy
import torch
import torch.nn as nn
import warnings
import argparse
from typing import Dict, Any, Optional, Tuple

# å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬çš„å¯¼å…¥
try:
    from torch.quantization import prepare, convert, prepare_qat
    from torch.quantization import get_default_qconfig, get_default_qat_qconfig
    from torch.quantization import QuantStub, DeQuantStub
except ImportError:
    print("âŒ PyTorché‡åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥PyTorchç‰ˆæœ¬")
    sys.exit(1)

warnings.filterwarnings("ignore")

# æ·»åŠ openpointsåˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'openpoints'))

try:
    from openpoints.models import build_model_from_cfg
    from openpoints.utils import EasyConfig
    from openpoints.dataset import build_dataloader_from_cfg
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥PointNeXtæ¨¡å—ï¼Œè¯·æ£€æŸ¥å®‰è£…")
    sys.exit(1)


class EagerQuantizationWrapper(nn.Module):
    """
    Eageræ¨¡å¼é‡åŒ–åŒ…è£…å™¨
    ä¸ºPointNeXtæ¨¡å‹æ·»åŠ é‡åŒ–/åé‡åŒ–æ“ä½œ
    """
    
    def __init__(self, model: nn.Module):
        super().__init__()
        self.quant = QuantStub()      # è¾“å…¥é‡åŒ–
        self.model = model            # åŸå§‹æ¨¡å‹
        self.dequant = DeQuantStub()  # è¾“å‡ºåé‡åŒ–
        
    def forward(self, data):
        # å¤„ç†PointNeXtçš„æ ‡å‡†æ•°æ®æ ¼å¼
        if isinstance(data, dict):
            # é‡åŒ–ä½ç½®ä¿¡æ¯
            if 'pos' in data and data['pos'] is not None:
                data['pos'] = self.quant(data['pos'])
            # é‡åŒ–ç‰¹å¾ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'x' in data and data['x'] is not None:
                data['x'] = self.quant(data['x'])
            
            # æ¨¡å‹å‰å‘ä¼ æ’­
            output = self.model(data)
            
            # åé‡åŒ–è¾“å‡º
            if isinstance(output, torch.Tensor):
                output = self.dequant(output)
            elif isinstance(output, dict):
                # å¤„ç†åˆ†ç±»è¾“å‡º
                if 'logits' in output:
                    output['logits'] = self.dequant(output['logits'])
                # å¤„ç†å…¶ä»–å¯èƒ½çš„è¾“å‡ºæ ¼å¼
                elif 'out' in output:
                    output['out'] = self.dequant(output['out'])
                
        else:
            # å…¼å®¹ç®€å•tensorè¾“å…¥
            if isinstance(data, torch.Tensor):
                # å‡è®¾æ˜¯ [B, N, 3] æˆ– [N, 3] æ ¼å¼ï¼Œè½¬æ¢ä¸ºPointNeXtæœŸæœ›çš„å­—å…¸æ ¼å¼
                if data.dim() == 2:  # [N, 3]
                    data = {'pos': self.quant(data)}
                elif data.dim() == 3:  # [B, N, 3]
                    data = {'pos': self.quant(data.squeeze(0))}
                else:
                    data = self.quant(data)
            
            output = self.model(data)
            
            if isinstance(output, torch.Tensor):
                output = self.dequant(output)
            elif isinstance(output, dict) and 'logits' in output:
                output['logits'] = self.dequant(output['logits'])
                
        return output


class PointNeXtEagerQuantizer:
    """
    PointNeXt Eageræ¨¡å¼é‡åŒ–å™¨
    æ”¯æŒé™æ€é‡åŒ–å’ŒQATé‡åŒ–ï¼ˆä»…ä½¿ç”¨çœŸå®ModelNet40æ•°æ®ï¼‰
    """
    
    def __init__(self, config_path: str, pretrained_path: str = None, device: str = 'cuda'):
        self.config_path = config_path
        self.pretrained_path = pretrained_path
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½®
        self.cfg = EasyConfig()
        self.cfg.load(config_path, recursive=True)
        
        print(f"ğŸ¯ Eageræ¨¡å¼é‡åŒ–å™¨åˆå§‹åŒ–ï¼ˆä»…ä½¿ç”¨çœŸå®ModelNet40æ•°æ®ï¼‰")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_path}")
        print(f"ğŸ”§ è®¾å¤‡: {self.device}")
        
    def build_model(self) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        print("ğŸ—ï¸  æ„å»ºæ¨¡å‹...")
        
        # æ„å»ºåŸå§‹æ¨¡å‹
        model = build_model_from_cfg(self.cfg.model)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if self.pretrained_path and os.path.exists(self.pretrained_path):
            print(f"ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæƒé‡: {self.pretrained_path}")
            checkpoint = torch.load(self.pretrained_path, map_location='cpu')
            if 'model' in checkpoint:
                model.load_state_dict(checkpoint['model'])
            else:
                model.load_state_dict(checkpoint)
        else:
            print("ğŸ“¦ ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
        
        return model
    
    def prepare_calibration_data(self) -> torch.utils.data.DataLoader:
        """å‡†å¤‡æ ¡å‡†æ•°æ® - ä»…ä½¿ç”¨çœŸå®çš„ModelNet40æ•°æ®"""
        print(f"ğŸ“Š å‡†å¤‡ModelNet40æ ¡å‡†æ•°æ®...")
        
        # ä»é»˜è®¤ModelNet40é…ç½®æ–‡ä»¶åŠ è½½æ•°æ®é›†é…ç½®
        modelnet_cfg = EasyConfig()
        modelnet_cfg.load('cfgs/modelnet40ply2048/default.yaml')
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„dataseté…ç½®
        dataset_cfg = modelnet_cfg.dataset
        dataloader_cfg = modelnet_cfg.dataloader
        datatransforms_cfg = modelnet_cfg.datatransforms
        
        # ä¿®æ”¹batch_sizeä¸º1ç”¨äºæ ¡å‡†
        batch_size = 1
        
        print("ğŸ”„ æ„å»ºModelNet40æ•°æ®åŠ è½½å™¨...")
        print(f"   æ•°æ®è·¯å¾„: {dataset_cfg.common.data_dir}")
        print(f"   ç‚¹æ•°: {dataset_cfg.train.num_points}")
        
        # æ„å»ºæ•°æ®åŠ è½½å™¨
        dataloader = build_dataloader_from_cfg(
            batch_size=batch_size,
            dataset_cfg=dataset_cfg,
            dataloader_cfg=dataloader_cfg,
            datatransforms_cfg=datatransforms_cfg,
            split='train',
            distributed=False  # ä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
        )
        
        print("âœ… æˆåŠŸæ„å»ºModelNet40æ•°æ®åŠ è½½å™¨")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        print("ğŸ” æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
        test_iter = iter(dataloader)
        sample_batch = next(test_iter)
        print(f"   æ ·æœ¬æ ¼å¼: {type(sample_batch)}")
        if isinstance(sample_batch, (list, tuple)):
            data, label = sample_batch
            print(f"   æ•°æ®ç±»å‹: {type(data)}")
            print(f"   æ•°æ®å½¢çŠ¶: {data.shape if hasattr(data, 'shape') else 'N/A'}")
            print(f"   æ ‡ç­¾å½¢çŠ¶: {label.shape if hasattr(label, 'shape') else type(label)}")
        
        return dataloader
    
    def _create_qconfig_dict(self, method: str = 'static') -> Dict[str, Any]:
        """
        åˆ›å»ºé‡åŒ–é…ç½®å­—å…¸
        Eageræ¨¡å¼çš„é…ç½®æ›´ç®€å•ï¼Œä¸éœ€è¦å¤„ç†FXå…¼å®¹æ€§é—®é¢˜
        """
        if method == 'qat':
            default_qconfig = get_default_qat_qconfig('fbgemm')
        else:
            default_qconfig = get_default_qconfig('fbgemm')
        
        qconfig_dict = {
            '': default_qconfig  # å…¨å±€é»˜è®¤é…ç½®
        }
        
        print(f"ğŸ“‹ é‡åŒ–é…ç½®: {method}æ¨¡å¼")
        print(f"   é»˜è®¤qconfig: {default_qconfig}")
        
        return qconfig_dict
    
    def _detect_task_type(self) -> str:
        """æ£€æµ‹ä»»åŠ¡ç±»å‹"""
        config_path_lower = self.config_path.lower()
        if 'classification' in config_path_lower or 'modelnet' in config_path_lower:
            return 'classification'
        elif 'segmentation' in config_path_lower or 's3dis' in config_path_lower:
            return 'segmentation'
        else:
            return 'classification'  # é»˜è®¤
    
    def static_quantize(self, model: nn.Module, calibration_loader: torch.utils.data.DataLoader) -> nn.Module:
        """é™æ€é‡åŒ– (Post-Training Quantization)"""
        print("ğŸ”„ å¼€å§‹é™æ€é‡åŒ–...")
        
        # 1. åŒ…è£…æ¨¡å‹
        wrapped_model = EagerQuantizationWrapper(model)
        wrapped_model.eval()
        
        # 2. è®¾ç½®é‡åŒ–é…ç½®
        qconfig_dict = self._create_qconfig_dict('static')
        wrapped_model.qconfig = qconfig_dict['']
        
        # 3. å‡†å¤‡é‡åŒ–
        prepared_model = prepare(wrapped_model, inplace=False)
        prepared_model = prepared_model.to(self.device)
        
        print("ğŸ“Š ä½¿ç”¨çœŸå®ModelNet40æ•°æ®è¿›è¡Œæ¨¡å‹æ ¡å‡†...")
        # 4. æ ¡å‡†é˜¶æ®µ
        with torch.no_grad():
            for i, batch in enumerate(calibration_loader):
                if i >= 50:  # é™åˆ¶æ ¡å‡†æ ·æœ¬æ•°é‡
                    break
                
                try:
                    # å¤„ç†ModelNet40æ•°æ®æ ¼å¼
                    if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                        data, _ = batch
                        # è½¬æ¢ä¸ºPointNeXtæœŸæœ›çš„å­—å…¸æ ¼å¼
                        if isinstance(data, torch.Tensor):
                            # dataæ˜¯ [B, N, 3] æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸
                            data = {'pos': data.squeeze(0).to(self.device)}  # å»æ‰batchç»´åº¦
                        elif isinstance(data, dict):
                            for key in data:
                                if isinstance(data[key], torch.Tensor):
                                    data[key] = data[key].to(self.device)
                    else:
                        print(f"âš ï¸  æœªçŸ¥çš„æ•°æ®æ ¼å¼: {type(batch)}")
                        continue
                    
                    # å‰å‘ä¼ æ’­è¿›è¡Œæ ¡å‡†
                    _ = prepared_model(data)
                    
                    if i % 10 == 0:
                        print(f"   æ ¡å‡†è¿›åº¦: {i+1}/50")
                        
                except Exception as e:
                    print(f"âš ï¸  æ ¡å‡†æ ·æœ¬ {i} å¤±è´¥: {e}")
                    if i < 5:  # å‰å‡ ä¸ªæ ·æœ¬å¤±è´¥æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                        print(f"      æ‰¹æ¬¡ç±»å‹: {type(batch)}")
                        if isinstance(batch, (list, tuple)):
                            print(f"      æ‰¹æ¬¡é•¿åº¦: {len(batch)}")
                    continue
        
        # 5. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        print("ğŸ”„ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
        # è½¬æ¢å¿…é¡»åœ¨CPUä¸Šè¿›è¡Œ
        prepared_model_cpu = prepared_model.cpu()
        quantized_model = convert(prepared_model_cpu, inplace=False)
        
        print("âœ… é™æ€é‡åŒ–å®Œæˆ")
        return quantized_model
    
    def qat_quantize(self, model: nn.Module, train_loader: torch.utils.data.DataLoader, 
                     num_epochs: int = 3, lr: float = 0.0001) -> nn.Module:
        """QATé‡åŒ– (Quantization-Aware Training)"""
        print(f"ğŸ”„ å¼€å§‹QATé‡åŒ– (è®­ç»ƒ{num_epochs}ä¸ªepoch)...")
        
        # 1. åŒ…è£…æ¨¡å‹
        wrapped_model = EagerQuantizationWrapper(model)
        
        # 2. è®¾ç½®é‡åŒ–é…ç½®
        qconfig_dict = self._create_qconfig_dict('qat')
        wrapped_model.qconfig = qconfig_dict['']
        
        # 3. å‡†å¤‡QAT
        prepared_model = prepare_qat(wrapped_model, inplace=False)
        prepared_model = prepared_model.to(self.device)
        
        # 4. QATè®­ç»ƒ
        task_type = self._detect_task_type()
        optimizer = torch.optim.Adam(prepared_model.parameters(), lr=lr)
        
        if task_type == 'classification':
            criterion = nn.CrossEntropyLoss()
        else:
            criterion = nn.CrossEntropyLoss(ignore_index=-1)
        
        print("ğŸ¯ å¼€å§‹QATè®­ç»ƒ...")
        for epoch in range(num_epochs):
            prepared_model.train()
            total_loss = 0
            num_batches = 0
            
            for i, batch in enumerate(train_loader):
                if i >= 30:  # é™åˆ¶è®­ç»ƒæ‰¹æ¬¡
                    break
                
                try:
                    optimizer.zero_grad()
                    
                    # å¤„ç†è¾“å…¥æ•°æ®
                    if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                        data, targets = batch
                        # è½¬æ¢ä¸ºPointNeXtæœŸæœ›çš„å­—å…¸æ ¼å¼
                        if isinstance(data, torch.Tensor):
                            data = {'pos': data.squeeze(0).to(self.device)}
                        elif isinstance(data, dict):
                            for key in data:
                                if isinstance(data[key], torch.Tensor):
                                    data[key] = data[key].to(self.device)
                        
                        targets = targets.to(self.device)
                    else:
                        continue
                    
                    # å‰å‘ä¼ æ’­
                    outputs = prepared_model(data)
                    
                    # è®¡ç®—æŸå¤±
                    if isinstance(outputs, dict) and 'logits' in outputs:
                        outputs = outputs['logits']
                    
                    if task_type == 'classification':
                        loss = criterion(outputs, targets.long().squeeze())
                    else:
                        # åˆ†å‰²ä»»åŠ¡
                        outputs = outputs.view(-1, outputs.shape[-1])
                        targets = targets.view(-1)
                        valid_mask = targets != -1
                        if valid_mask.sum() > 0:
                            loss = criterion(outputs[valid_mask], targets[valid_mask].long())
                        else:
                            continue
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    if i % 10 == 0:
                        print(f"   Epoch {epoch+1}/{num_epochs}, Batch {i+1}, Loss: {loss.item():.4f}")
                        
                except Exception as e:
                    print(f"âš ï¸  è®­ç»ƒæ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue
            
            avg_loss = total_loss / max(num_batches, 1)
            print(f"ğŸ“Š Epoch {epoch+1}/{num_epochs} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # 5. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        print("ğŸ”„ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
        prepared_model.eval()
        prepared_model_cpu = prepared_model.cpu()
        quantized_model = convert(prepared_model_cpu, inplace=False)
        
        print("âœ… QATé‡åŒ–å®Œæˆ")
        return quantized_model
    
    def evaluate_model(self, model: nn.Module, test_loader: torch.utils.data.DataLoader, 
                      name: str = "æ¨¡å‹") -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼°{name}...")
        
        # æ£€æµ‹æ˜¯å¦ä¸ºé‡åŒ–æ¨¡å‹
        is_quantized = any(hasattr(m, '_weight_bias') or 'quantized' in str(type(m)).lower() 
                          for m in model.modules())
        
        if is_quantized:
            print(f"ğŸ”„ æ£€æµ‹åˆ°é‡åŒ–æ¨¡å‹ï¼Œåœ¨CPUä¸Šè¯„ä¼°...")
            model = model.cpu()
            eval_device = torch.device('cpu')
        else:
            model = model.to(self.device)
            eval_device = self.device
        
        model.eval()
        total_samples = 0
        correct = 0
        total_loss = 0
        
        task_type = self._detect_task_type()
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for i, batch in enumerate(test_loader):
                if i >= 20:  # é™åˆ¶è¯„ä¼°æ ·æœ¬
                    break
                
                try:
                    # å¤„ç†ModelNet40æ•°æ®æ ¼å¼
                    if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                        data, targets = batch
                        # è½¬æ¢ä¸ºPointNeXtæœŸæœ›çš„å­—å…¸æ ¼å¼
                        if isinstance(data, torch.Tensor):
                            data = {'pos': data.squeeze(0).to(eval_device)}
                        elif isinstance(data, dict):
                            for key in data:
                                if isinstance(data[key], torch.Tensor):
                                    data[key] = data[key].to(eval_device)
                        
                        targets = targets.to(eval_device)
                    else:
                        print(f"âš ï¸  æœªçŸ¥çš„è¯„ä¼°æ•°æ®æ ¼å¼: {type(batch)}")
                        continue
                    
                    # å‰å‘ä¼ æ’­
                    outputs = model(data)
                    
                    if isinstance(outputs, dict) and 'logits' in outputs:
                        outputs = outputs['logits']
                    
                    # è®¡ç®—å‡†ç¡®ç‡å’ŒæŸå¤±
                    if task_type == 'classification':
                        loss = criterion(outputs, targets.long().squeeze())
                        _, predicted = torch.max(outputs.data, 1)
                        correct += (predicted == targets.squeeze()).sum().item()
                        total_samples += targets.size(0)
                    else:
                        # åˆ†å‰²ä»»åŠ¡
                        outputs_flat = outputs.view(-1, outputs.shape[-1])
                        targets_flat = targets.view(-1)
                        valid_mask = targets_flat != -1
                        
                        if valid_mask.sum() > 0:
                            loss = criterion(outputs_flat[valid_mask], targets_flat[valid_mask].long())
                            _, predicted = torch.max(outputs_flat[valid_mask], 1)
                            correct += (predicted == targets_flat[valid_mask]).sum().item()
                            total_samples += valid_mask.sum().item()
                    
                    total_loss += loss.item()
                    
                except Exception as e:
                    print(f"âš ï¸  è¯„ä¼°æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                    continue
        
        accuracy = 100 * correct / max(total_samples, 1)
        avg_loss = total_loss / max(i + 1, 1)
        
        print(f"ğŸ“Š {name}ç»“æœ:")
        print(f"   å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"   è¯„ä¼°æ ·æœ¬æ•°: {total_samples}")
        
        return {
            'accuracy': accuracy,
            'loss': avg_loss,
            'samples': total_samples
        }
    
    def compare_models(self, original_model: nn.Module, quantized_model: nn.Module, 
                      test_loader: torch.utils.data.DataLoader) -> Dict[str, Any]:
        """å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹"""
        print("ğŸ” å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹...")
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹
        original_results = self.evaluate_model(original_model, test_loader, "åŸå§‹æ¨¡å‹")
        
        # è¯„ä¼°é‡åŒ–æ¨¡å‹
        quantized_results = self.evaluate_model(quantized_model, test_loader, "é‡åŒ–æ¨¡å‹")
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        def get_model_size(model):
            torch.save(model.state_dict(), 'temp_model.pth')
            size = os.path.getsize('temp_model.pth')
            os.remove('temp_model.pth')
            return size
        
        original_size = get_model_size(original_model)
        quantized_size = get_model_size(quantized_model)
        compression_ratio = original_size / quantized_size
        
        # æ€»ç»“å¯¹æ¯”ç»“æœ
        comparison = {
            'original': original_results,
            'quantized': quantized_results,
            'accuracy_drop': original_results['accuracy'] - quantized_results['accuracy'],
            'original_size_mb': original_size / (1024 * 1024),
            'quantized_size_mb': quantized_size / (1024 * 1024),
            'compression_ratio': compression_ratio
        }
        
        print("\n" + "="*50)
        print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
        print("="*50)
        print(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡:    {original_results['accuracy']:.2f}%")
        print(f"é‡åŒ–æ¨¡å‹å‡†ç¡®ç‡:    {quantized_results['accuracy']:.2f}%")
        print(f"å‡†ç¡®ç‡ä¸‹é™:        {comparison['accuracy_drop']:.2f}%")
        print(f"åŸå§‹æ¨¡å‹å¤§å°:      {comparison['original_size_mb']:.2f} MB")
        print(f"é‡åŒ–æ¨¡å‹å¤§å°:      {comparison['quantized_size_mb']:.2f} MB")
        print(f"å‹ç¼©æ¯”:           {compression_ratio:.2f}x")
        print("="*50)
        
        return comparison
    
    def save_quantized_model(self, model: nn.Module, save_path: str):
        """ä¿å­˜é‡åŒ–æ¨¡å‹"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        try:
            # ç¡®ä¿æ¨¡å‹åœ¨CPUä¸Šè¿›è¡Œä¿å­˜
            model_cpu = model.cpu()
            torch.save(model_cpu.state_dict(), save_path)
            print(f"ğŸ’¾ é‡åŒ–æ¨¡å‹å·²ä¿å­˜: {save_path}")
            
            # æ‰“å°æ–‡ä»¶å¤§å°
            size_mb = os.path.getsize(save_path) / (1024 * 1024)
            print(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='PointNeXt Eageræ¨¡å¼é‡åŒ–ï¼ˆä»…ä½¿ç”¨çœŸå®ModelNet40æ•°æ®ï¼‰')
    parser.add_argument('--cfg', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--method', type=str, choices=['static', 'qat', 'compare'], 
                       default='static', help='é‡åŒ–æ–¹æ³•')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    parser.add_argument('--epochs', type=int, default=3, help='QATè®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.0001, help='QATå­¦ä¹ ç‡')
    parser.add_argument('--pretrained', type=str, help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_dir', type=str, default='quantized_models', help='ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹PointNeXt Eageræ¨¡å¼é‡åŒ– (ä»…ä½¿ç”¨çœŸå®ModelNet40æ•°æ®)")
    print("="*60)
    
    # åˆå§‹åŒ–é‡åŒ–å™¨
    quantizer = PointNeXtEagerQuantizer(
        config_path=args.cfg,
        pretrained_path=args.pretrained,
        device=args.device
    )
    
    # æ„å»ºæ¨¡å‹
    model = quantizer.build_model()
    
    # å‡†å¤‡æ•°æ®
    calibration_loader = quantizer.prepare_calibration_data()
    test_loader = calibration_loader  # ä½¿ç”¨ç›¸åŒæ•°æ®è¿›è¡Œæµ‹è¯•ï¼ˆæ¼”ç¤ºç”¨ï¼‰
    
    if args.method == 'static':
        # é™æ€é‡åŒ–
        quantized_model = quantizer.static_quantize(model, calibration_loader)
        
        # ä¿å­˜æ¨¡å‹
        save_path = os.path.join(args.save_dir, 'quantized_model_static_eager.pth')
        quantizer.save_quantized_model(quantized_model, save_path)
        
        # å¯¹æ¯”æ€§èƒ½
        comparison = quantizer.compare_models(model, quantized_model, test_loader)
        
    elif args.method == 'qat':
        # QATé‡åŒ–
        quantized_model = quantizer.qat_quantize(
            model, calibration_loader, 
            num_epochs=args.epochs, 
            lr=args.lr
        )
        
        # ä¿å­˜æ¨¡å‹
        save_path = os.path.join(args.save_dir, 'quantized_model_qat_eager.pth')
        quantizer.save_quantized_model(quantized_model, save_path)
        
        # å¯¹æ¯”æ€§èƒ½
        comparison = quantizer.compare_models(model, quantized_model, test_loader)
        
    elif args.method == 'compare':
        # æ¯”è¾ƒä¸¤ç§é‡åŒ–æ–¹æ³•
        print("ğŸ”„ æ¯”è¾ƒé™æ€é‡åŒ–å’ŒQAT...")
        
        static_model = quantizer.static_quantize(model, calibration_loader)
        qat_model = quantizer.qat_quantize(copy.deepcopy(model), calibration_loader, args.epochs, args.lr)
        
        print("\n" + "="*60)
        print("ğŸ“Š é™æ€é‡åŒ– vs QATé‡åŒ–å¯¹æ¯”")
        print("="*60)
        
        static_comparison = quantizer.compare_models(model, static_model, test_loader)
        print(f"\né™æ€é‡åŒ–ç»“æœ:")
        print(f"  å‡†ç¡®ç‡ä¸‹é™: {static_comparison['accuracy_drop']:.2f}%")
        print(f"  å‹ç¼©æ¯”: {static_comparison['compression_ratio']:.2f}x")
        
        qat_comparison = quantizer.compare_models(copy.deepcopy(model), qat_model, test_loader)
        print(f"\nQATé‡åŒ–ç»“æœ:")
        print(f"  å‡†ç¡®ç‡ä¸‹é™: {qat_comparison['accuracy_drop']:.2f}%")
        print(f"  å‹ç¼©æ¯”: {qat_comparison['compression_ratio']:.2f}x")
        
        # ä¿å­˜ä¸¤ä¸ªæ¨¡å‹
        static_path = os.path.join(args.save_dir, 'quantized_model_static_eager.pth')
        qat_path = os.path.join(args.save_dir, 'quantized_model_qat_eager.pth')
        quantizer.save_quantized_model(static_model, static_path)
        quantizer.save_quantized_model(qat_model, qat_path)
    
    print("âœ… é‡åŒ–å®Œæˆ!")


if __name__ == "__main__":
    main()