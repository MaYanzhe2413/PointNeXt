#!/usr/bin/env python3
"""
æµ‹è¯•é‡åŒ–æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import torch.nn as nn
from quantize_eager import EagerQuantizationWrapper
from openpoints.models import build_model_from_cfg
from openpoints.utils import EasyConfig

def test_quantized_model():
    print("ğŸ§ª æµ‹è¯•é‡åŒ–æ¨¡å‹...")
    
    # 1. æ„å»ºåŸå§‹æ¨¡å‹
    cfg = EasyConfig()
    cfg.load('cfgs/modelnet40ply2048/pointnet++.yaml', recursive=True)
    original_model = build_model_from_cfg(cfg.model)
    
    print(f"âœ… åŸå§‹æ¨¡å‹æ„å»ºå®Œæˆ: {type(original_model).__name__}")
    
    # 2. åŠ è½½é‡åŒ–æ¨¡å‹
    try:
        # åŒ…è£…æ¨¡å‹å¹¶è®¾ç½®é‡åŒ–é…ç½®
        wrapped_model = EagerQuantizationWrapper(original_model)
        from torch.quantization import get_default_qconfig, prepare, convert
        wrapped_model.qconfig = get_default_qconfig('fbgemm')
        
        # å‡†å¤‡é‡åŒ–
        prepared_model = prepare(wrapped_model, inplace=False)
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = convert(prepared_model, inplace=False)
        
        print("âœ… é‡åŒ–æ¨¡å‹æ„å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ é‡åŒ–æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•æ¨ç†
    print("ğŸ”„ æµ‹è¯•æ¨¡å‹æ¨ç†...")
    
    # æµ‹è¯•æ•°æ®
    test_data = torch.randn(2, 1024, 3)  # [B, N, 3]
    
    try:
        # åŸå§‹æ¨¡å‹æ¨ç†
        original_model.eval()
        with torch.no_grad():
            original_output = original_model(test_data)
        
        print(f"âœ… åŸå§‹æ¨¡å‹æ¨ç†æˆåŠŸ: è¾“å‡ºå½¢çŠ¶ {original_output.shape}")
        
        # é‡åŒ–æ¨¡å‹æ¨ç†
        quantized_model.eval()
        with torch.no_grad():
            quantized_output = quantized_model(test_data)
        
        print(f"âœ… é‡åŒ–æ¨¡å‹æ¨ç†æˆåŠŸ: è¾“å‡ºå½¢çŠ¶ {quantized_output.shape}")
        
        # æ¯”è¾ƒè¾“å‡º
        if original_output.shape == quantized_output.shape:
            print("âœ… è¾“å‡ºå½¢çŠ¶ä¸€è‡´")
            
            # è®¡ç®—å·®å¼‚
            mse = torch.mean((original_output - quantized_output) ** 2).item()
            print(f"ğŸ“Š å‡æ–¹è¯¯å·®: {mse:.6f}")
            
            if mse < 1.0:  # å…è®¸ä¸€å®šçš„é‡åŒ–è¯¯å·®
                print("âœ… è¾“å‡ºå·®å¼‚åœ¨åˆç†èŒƒå›´å†…")
                return True
            else:
                print("âš ï¸  è¾“å‡ºå·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¥½çš„æ ¡å‡†")
                return True  # ä»ç„¶ç®—æˆåŠŸï¼Œå› ä¸ºå½¢çŠ¶æ­£ç¡®
        else:
            print(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸ä¸€è‡´: {original_output.shape} vs {quantized_output.shape}")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_size():
    print("\nğŸ“¦ æµ‹è¯•æ¨¡å‹å¤§å°...")
    
    try:
        # æµ‹è¯•ä¿å­˜çš„é‡åŒ–æ¨¡å‹
        if torch.cuda.is_available():
            device = 'cuda'
        else:
            device = 'cpu'
            
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æ„å»ºåŸå§‹æ¨¡å‹
        cfg = EasyConfig()
        cfg.load('cfgs/modelnet40ply2048/pointnet++.yaml', recursive=True)
        original_model = build_model_from_cfg(cfg.model)
        
        # ä¿å­˜åŸå§‹æ¨¡å‹
        torch.save(original_model.state_dict(), 'temp_original.pth')
        original_size = torch.load('temp_original.pth', map_location='cpu')
        
        # è®¡ç®—å‚æ•°æ•°é‡
        original_params = sum(p.numel() for p in original_model.parameters())
        
        print(f"ğŸ“Š åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡: {original_params:,}")
        
        # æ£€æŸ¥é‡åŒ–æ¨¡å‹æ˜¯å¦å­˜åœ¨
        import os
        if os.path.exists('quantized_models/quantized_model_static_eager.pth'):
            quantized_size = os.path.getsize('quantized_models/quantized_model_static_eager.pth')
            original_file_size = os.path.getsize('temp_original.pth')
            
            print(f"ğŸ“¦ åŸå§‹æ¨¡å‹æ–‡ä»¶å¤§å°: {original_file_size / (1024*1024):.2f} MB")
            print(f"ğŸ“¦ é‡åŒ–æ¨¡å‹æ–‡ä»¶å¤§å°: {quantized_size / (1024*1024):.2f} MB")
            print(f"ğŸ¯ å‹ç¼©æ¯”: {original_file_size / quantized_size:.2f}x")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove('temp_original.pth')
            
            return True
        else:
            print("âŒ é‡åŒ–æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹å¤§å°æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("="*50)
    print("ğŸ§ª PointNeXt Eageræ¨¡å¼é‡åŒ–æµ‹è¯•")
    print("="*50)
    
    # æµ‹è¯•é‡åŒ–æ¨¡å‹åŠŸèƒ½
    inference_success = test_quantized_model()
    
    # æµ‹è¯•æ¨¡å‹å¤§å°
    size_success = test_model_size()
    
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*50)
    print(f"æ¨ç†æµ‹è¯•:     {'âœ… é€šè¿‡' if inference_success else 'âŒ å¤±è´¥'}")
    print(f"å¤§å°æµ‹è¯•:     {'âœ… é€šè¿‡' if size_success else 'âŒ å¤±è´¥'}")
    
    if inference_success and size_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Eageræ¨¡å¼é‡åŒ–å·¥ä½œæ­£å¸¸ï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å…·ä½“é—®é¢˜ã€‚")
    
    print("="*50)